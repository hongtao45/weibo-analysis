{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本预处理 流程（原来的一样，不做变化）\n",
    "\n",
    "1. 文本去噪\n",
    "2. 中文分词\n",
    "3. 过滤停用词\n",
    "4. 词性标注 【没有特殊需求，咱们就不用做了\n",
    "5. 文本去重 【考虑发表的用户，**官方号**\n",
    "6. 文本标记【手动整理模型输入数据\n",
    "7. 特征词选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>微博正文</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>【面对暴雨，请收好这份#避险生存指南#】#蜀黍带你涨姿势#近日来，河南两小时暴雨记录又被刷新...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#河南高速路况#截至2021年9月9日,19:00，目前省内高速通行情况：1、因收费站临时抢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>郑州大暴雨灾害这波，今天回来又看到一交通事故，一个大叔满身血淋淋坐在车前，前几日杭州电瓶车骑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>【暖心！#交警暴雨中执勤频频被路人送伞#】6月10日17时许，浙江温州泰顺县一路口严重拥堵，...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>【湖南多地迎暴雨！这些收费站仍在管制中】今日晚间，湖南部分地区突降暴雨，905中国交通广播派...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                微博正文\n",
       "0  【面对暴雨，请收好这份#避险生存指南#】#蜀黍带你涨姿势#近日来，河南两小时暴雨记录又被刷新...\n",
       "1  #河南高速路况#截至2021年9月9日,19:00，目前省内高速通行情况：1、因收费站临时抢...\n",
       "2  郑州大暴雨灾害这波，今天回来又看到一交通事故，一个大叔满身血淋淋坐在车前，前几日杭州电瓶车骑...\n",
       "3  【暖心！#交警暴雨中执勤频频被路人送伞#】6月10日17时许，浙江温州泰顺县一路口严重拥堵，...\n",
       "4  【湖南多地迎暴雨！这些收费站仍在管制中】今日晚间，湖南部分地区突降暴雨，905中国交通广播派..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入包\n",
    "# 导入数据\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import pkuseg\n",
    "import jieba\n",
    "\n",
    "data = pd.read_csv(\"../02Data/完整数据_暴雨_交通.csv\")\n",
    "df = data.sample(n=500, replace=False, random_state=1) # 指定随机种子，保证都是同一个数据组合\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "tweets = df.loc[:99, ['微博正文']].astype(\"string\") # 取100条数据就好了\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   微博正文    100 non-null    string\n",
      "dtypes: string(1)\n",
      "memory usage: 928.0 bytes\n"
     ]
    }
   ],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 文本去噪\n",
    "\n",
    "微博中的特殊符号，对于算法的训练来说不太有利\n",
    "\n",
    "[Python正则表达式清洗微博文本特殊符号(网址, @, 表情符等)](https://blog.csdn.net/blmoistawinde/article/details/103648044)\n",
    "\n",
    "[附：表达式全集（正则表达式手册）](https://blog.csdn.net/qq_33472765/article/details/80785441)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_noise(text):\n",
    "    \"\"\"\n",
    "    text 待去噪的原始文本\n",
    "    \"\"\"\n",
    "    text = text.replace(u'\\xa0', u' ')      # 去除 \\xa0     不间断空白符 \n",
    "    text = text.replace(u'\\u3000', u' ')    # 去除 \\u3000   全角的空白符\n",
    "    \n",
    "    text = re.sub(r\"(回复)?(//)?\\s*@\\S*?\\s*(，|:| |$)\", \"\", text)  # 去除正文中的@和回复/转发中的用户名\n",
    "    text = re.sub(r\"\\[\\S+\\]\", \"\", text)     # 去除表情符号\n",
    "    # text = re.sub(r\"#\\S+\\s*#\", \"\", text)    # 去除话题内容 “#content#”\n",
    "    # text = re.sub(r\"【\\S+\\s*】\", \"\", text)  # 分类标签“【content】”\n",
    "\n",
    "    URL_REGEX = re.compile(\n",
    "        r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
    "        re.IGNORECASE)\n",
    "    text = re.sub(URL_REGEX, \"\", text)      # 去除网址\n",
    "    \n",
    "    EMAIL_REGEX = re.compile(r\"[-a-z0-9_.]+@(?:[-a-z0-9]+\\.)+[a-z]{2,6}\", re.IGNORECASE)\n",
    "    text = re.sub(EMAIL_REGEX, \"\", text)    # 去除邮件 \n",
    "    \n",
    "    text = text.replace(\"转发微博\", \"\")      # 去除无意义的词语\n",
    "    text = text.replace(\"网页链接\", \"\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)        # 合并正文中过多的空格\n",
    "\n",
    "    text = re.sub(r\"\\d{2,4}年|\\d{1,2}月|\\d{1,2}日|\\d{1,2}时|\\d{1,2}分| \\d{1,2}点\", \"\", text) # 去除 日期 时间\n",
    "    text = re.sub(r\"\\d\", \"\", text)\n",
    "    text = re.sub(r'[a-zA-Z]',\"\", text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "匹配次数：1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 之间的东西也会被删\n",
    "text1 = \"#河南高速路况#截至2021年9月9日,19:00，目前省内高速通行情况高速服务电话12328求助。#河南暴雨救援#\"\n",
    "\n",
    "text1, nu = re.subn(r\"(#)?\\S+\\s*(#)?\", \"\", text1)    # 去除话题内容 “#content#”\n",
    "print(\"匹配次数：{}\".format(nu))\n",
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>微博正文</th>\n",
       "      <th>去噪</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>【面对暴雨，请收好这份#避险生存指南#】#蜀黍带你涨姿势#近日来，河南两小时暴雨记录又被刷新...</td>\n",
       "      <td>【面对暴雨，请收好这份#避险生存指南#】#蜀黍带你涨姿势#近日来，河南两小时暴雨记录又被刷新...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#河南高速路况#截至2021年9月9日,19:00，目前省内高速通行情况：1、因收费站临时抢...</td>\n",
       "      <td>#河南高速路况#截至,:，目前省内高速通行情况：、因收费站临时抢修施工，菏宝高速新乡东站（北...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>郑州大暴雨灾害这波，今天回来又看到一交通事故，一个大叔满身血淋淋坐在车前，前几日杭州电瓶车骑...</td>\n",
       "      <td>郑州大暴雨灾害这波，今天回来又看到一交通事故，一个大叔满身血淋淋坐在车前，前几日杭州电瓶车骑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>【暖心！#交警暴雨中执勤频频被路人送伞#】6月10日17时许，浙江温州泰顺县一路口严重拥堵，...</td>\n",
       "      <td>【暖心！#交警暴雨中执勤频频被路人送伞#】许，浙江温州泰顺县一路口严重拥堵，一名交警在疏导交...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>【湖南多地迎暴雨！这些收费站仍在管制中】今日晚间，湖南部分地区突降暴雨，905中国交通广播派...</td>\n",
       "      <td>【湖南多地迎暴雨！这些收费站仍在管制中】今日晚间，湖南部分地区突降暴雨，中国交通广播派出四路...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                微博正文  \\\n",
       "0  【面对暴雨，请收好这份#避险生存指南#】#蜀黍带你涨姿势#近日来，河南两小时暴雨记录又被刷新...   \n",
       "1  #河南高速路况#截至2021年9月9日,19:00，目前省内高速通行情况：1、因收费站临时抢...   \n",
       "2  郑州大暴雨灾害这波，今天回来又看到一交通事故，一个大叔满身血淋淋坐在车前，前几日杭州电瓶车骑...   \n",
       "3  【暖心！#交警暴雨中执勤频频被路人送伞#】6月10日17时许，浙江温州泰顺县一路口严重拥堵，...   \n",
       "4  【湖南多地迎暴雨！这些收费站仍在管制中】今日晚间，湖南部分地区突降暴雨，905中国交通广播派...   \n",
       "\n",
       "                                                  去噪  \n",
       "0  【面对暴雨，请收好这份#避险生存指南#】#蜀黍带你涨姿势#近日来，河南两小时暴雨记录又被刷新...  \n",
       "1  #河南高速路况#截至,:，目前省内高速通行情况：、因收费站临时抢修施工，菏宝高速新乡东站（北...  \n",
       "2  郑州大暴雨灾害这波，今天回来又看到一交通事故，一个大叔满身血淋淋坐在车前，前几日杭州电瓶车骑...  \n",
       "3  【暖心！#交警暴雨中执勤频频被路人送伞#】许，浙江温州泰顺县一路口严重拥堵，一名交警在疏导交...  \n",
       "4  【湖南多地迎暴雨！这些收费站仍在管制中】今日晚间，湖南部分地区突降暴雨，中国交通广播派出四路...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"去噪\"] = tweets[\"微博正文\"].apply(clear_noise).astype(\"string\")\n",
    "tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   微博正文    100 non-null    string\n",
      " 1   去噪      100 non-null    string\n",
      "dtypes: string(2)\n",
      "memory usage: 1.7 KB\n"
     ]
    }
   ],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 中文分词\n",
    "\n",
    "[pkuseg: 一个多领域中文分词工具包](https://github.com/lancopku/pkuseg-python)\n",
    "\n",
    "[jieba: 结巴中文分词](https://github.com/fxsjy/jieba)\n",
    "\n",
    "从github下载的用户则需要自己下载对应的预训练模型，并设置model_name字段为预训练模型路径。预训练模型可以在[release](https://github.com/lancopku/pkuseg-python/releases)部分下载。以下是对预训练模型的说明：\n",
    "\n",
    "- **news**: 在MSRA（新闻语料）上训练的模型。【可以测试\n",
    "- **web**: 在微博（网络文本语料）上训练的模型。【可以测试\n",
    "- **medicine**: 在医药领域上训练的模型。\n",
    "- **tourism**: 在旅游领域上训练的模型。\n",
    "- **mixed**: 混合数据集训练的通用模型。随pip包附带的是此模型。【可以测试\n",
    "\n",
    "我们还通过领域自适应的方法，利用维基百科的未标注数据实现了几个细领域预训练模型的自动构建以及通用模型的优化，这些模型目前仅可以在release中下载：\n",
    "\n",
    "- **art**: 在艺术与文化领域上训练的模型。\n",
    "- **entertainment**: 在娱乐与体育领域上训练的模型。\n",
    "- **science**: 在科学领域上训练的模型。\n",
    "- **default_v2**: 使用领域自适应方法得到的优化后的通用模型，相较于默认模型规模更大，但泛化性能更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '爱', '北京', '天安门']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# jieba分词\n",
    "\n",
    "def jieba_segment(str, use_paddle=True):\n",
    "    \"\"\"\n",
    "    jieba分词:\n",
    "    str         : 待分词的文本\n",
    "    use_paddle  : 是否使用paddle模型\n",
    "    \"\"\"\n",
    "    word_list = jieba.cut(str, use_paddle=use_paddle)   # 分词后返回一个列表  jieba.cut(）   返回的是一个迭代器\n",
    "    res = list(word_list)\n",
    "   \n",
    "    return res\n",
    "\n",
    "#! 测试\n",
    "jieba_segment('我爱北京天安门')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '爱', '北京', '天安门']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pkuseg 分词 \n",
    "\n",
    "def pkuseg_segment(str, m_name=\"web\", u_dict=\"default\"):\n",
    "    \"\"\"\n",
    "    pkuseg分词:\n",
    "    str     :待分词的文本\n",
    "    m_name  : 选择使用的预训练模型\n",
    "    u_dict  : 用户自定义的分词设置用户词典。\n",
    "    \"\"\"\n",
    "    # m_name= \"news\"\n",
    "    # m_name= \"mixed\"\n",
    "    # p_tag= True # 是否包含词性\n",
    "    seg = pkuseg.pkuseg(model_name=m_name, user_dict=u_dict)           # 以默认配置加载模型    \n",
    "    res = seg.cut(str)  # 进行分词\n",
    "   \n",
    "    return res\n",
    "\n",
    "#! 测试\n",
    "pkuseg_segment('我爱北京天安门')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets[\"jieba\"] = tweets[\"去噪\"].apply(jieba_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! 时间比较3min，轻易不尝试\n",
    "tweets[\"pkuseg\"] = tweets[\"去噪\"].apply(pkuseg_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   微博正文    100 non-null    string\n",
      " 1   去噪      100 non-null    string\n",
      " 2   jieba   100 non-null    object\n",
      " 3   pkuseg  100 non-null    object\n",
      "dtypes: object(2), string(2)\n",
      "memory usage: 3.2+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets.to_excel(\"../02Data/jieba_pkuseg.xlsx\")\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 过滤停用词\n",
    "\n",
    "- 支持新增停用词库\n",
    "- 考虑词性标准时的去停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['辽宁', '天气', '我省', '大部分', '地区', '出现', '强对流', '天气', '注意', '防范', '雷雨大风', '短时', '强降水', '影响', '摘要', '预计', '我省', '大部分', '地区', '出现', '雷电', '大风', '短时', '强降水', '冰雹', '强对流', '天气', '西部', '北部', '地区', '最大', '瞬时', '风力', '局部', '可达级', '以上', '西部', '个别', '乡镇', '街道', '降雨量', '毫米', '最大', '小时', '降雨量', '毫米', '西部', '东南部', '地区', '强对流', '天气', '主要', '影响', '时段', '中午', '夜间', '北部', '地区', '夜间', '注意', '防范', '强对流', '天气', '带来', '不利', '影响', '气象条件', '分析', '蒙古', '中部', '高空槽', '东移', '过程', '加深', '高空', '冷涡', '低层', '偏南', '急流', '建立', '辽宁', '输送', '暖湿', '空气', '整层', '大气', '降水量', '水汽', '条件', '充足', '有利于', '出现', '短时', '强降水', '西部', '北部', '地区', '稳定', '能量', '充分', '积累', '对流', '有效', '逐渐', '增强', '配合', '垂直', '切变', '有利于', '对流', '系统', '发展', '出现', '大风', '冰雹', '强对流', '天气', '入海', '气旋', '影响', '白天', '夜间', '我省', '海区', '出现', '大风', '天气', '强对流', '天气预报', '我省', '大部分', '地区', '强对流', '天气', '预计', '锦州', '阜新', '铁岭', '朝阳', '盘锦', '葫芦岛', '地区', '康平', '法库', '新民', '台安', '雷电', '大风', '短时', '强降水', '冰雹', '强对流', '天气', '最大', '瞬时', '风力', '局部', '可达级', '以上', '最大', '小时', '降雨量', '毫米', '丹东', '地区', '大连市', '旅顺', '金普', '新区', '普兰店', '长海', '庄河', '岫岩', '雷电', '短时', '强降水', '最大', '小时', '降雨量', '毫米', '地区', '雷电', '西部', '东南部', '地区', '强对流', '天气', '主要', '影响', '时段', '中午', '夜间', '北部', '地区', '夜间', '白天', '夜间', '海区', '东风', '东北风', '阵风', '影响', '建议', '强对流', '天气', '突发性', '局地', '性强', '提醒', '公众', '相关', '部门', '关注', '气象部门', '滚动', '更新', '发布', '预报', '预警', '信息', '组织', '做好', '防范', '应对', '工作', '建议', '我省', '出现', '强降雨', '预计', '西部', '地区', '出现', '强降雨', '降雨', '叠加', '作用', '影响', '发生', '山洪', '滑坡', '泥石流', '中小', '河流', '洪水', '可能性', '增大', '做好', '流域', '水库', '小城镇', '防汛', '重点', '人群', '避险', '转移', '工作', '加强', '地质灾害', '隐患', '山区', '城镇', '学校', '医院', '重要', '工程', '设施', '煤矿', '尾矿库', '监测', '防范', '当有', '雷电', '天气', '发生', '尽量避免', '户外活动', '室外', '人员', '不要', '树下', '电杆', '塔吊', '避雨', '防范', '雷电', '可能', '造成', '人员伤亡', '设备', '损失', '关注', '航空运输', '影响', '加强', '国省', '干道', '区县', '乡镇', '道路', '过河', '桥梁', '隧道', '涵洞', '城市', '低洼', '路段', '管控', '防范', '短时', '强降水', '造成', '城市', '内涝', '交通', '出行', '影响', '瞬时', '风力', '较大', '注意', '防范', '可能', '造成', '搭建', '倒塌', '高空作业', '水域', '作业', '设施', '农业', '海上', '航运', '影响', '检查', '加固', '塔吊', '脚手架', '户外', '广告牌', '公众', '外出', '不要', '广告牌', '临时', '搭建', '物等', '下面', '停留', '防止', '大风', '引发', '倒塌', '坠物', '触电', '事件', '海上', '航运', '需注意', '安全', '妥善', '保护', '易受', '冰雹', '袭击', '汽车', '室外', '物品', '设备', '防范', '冰雹', '农作物', '果蔬', '农业', '设施', '造成', '不利', '影响', '提前', '驱赶', '家禽', '牲畜', '进入', '顶棚', '场所', '土壤', '饱和', '地区', '提前', '疏通', '沟渠', '及时', '排除', '田间', '积水', '防范', '强降雨', '引发', '农田', '渍涝', '辽宁', '辽宁', '暴雨', '辽宁', '身边', '辽宁', '生活', '天气', '强对流', '天气']\n"
     ]
    }
   ],
   "source": [
    "def clear_stopword(word_ls, stopword_file, postag=False, user_file=\"../05stopwords/user_stopwords.txt\"):\n",
    "    \"\"\"\n",
    "    word_ls         :待去停用词的词汇列表【word 或者是 tuple(word, postag) 组成的list\n",
    "    stopword_file   :选择使用的停用词库\n",
    "    user_file       :用户自定义的停用词库\n",
    "    postag          :是否有词性标注\n",
    "    \"\"\"\n",
    "    with open(stopword_file, 'r', encoding='utf-8') as f1, open(user_file, 'r' , encoding='utf-8') as f2:    # \n",
    "        \n",
    "        stopword_ls = [word.strip('\\n') for word in f1.readlines()] # 默认词库\n",
    "        user_ls = [word.strip('\\n') for word in f2.readlines()]     # 自定义词库\n",
    "\n",
    "        stopword_ls.extend(user_ls)\n",
    "        \n",
    "        res = []\n",
    "        if postag: # 有词性标注\n",
    "            for word_tag in word_ls:\n",
    "                w, t = word_tag\n",
    "                if w not in stopword_ls and len(w) > 1: # 仅保留2个字符及以上的词\n",
    "                    res.append((w, t))\n",
    "        else:\n",
    "            for w in word_ls:\n",
    "                if w not in stopword_ls and len(w) > 1: # 仅保留2个字符及以上的词\n",
    "                    res.append(w)\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "#! 测试\n",
    "word_ls = tweets[\"jieba\"][9]\n",
    "stopword_file = \"../05stopwords/hit_stopwords.txt\"\n",
    "postag = False\n",
    "user_file = \"../05stopwords/user_stopwords.txt\"\n",
    "\n",
    "res = clear_stopword(word_ls, stopword_file, postag, user_file)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_file = \"../05stopwords/hit_stopwords.txt\"\n",
    "postag = False\n",
    "user_file = \"../05stopwords/user_stopwords.txt\"\n",
    "\n",
    "tweets[\"jieba_stop\"] = tweets[\"jieba\"].apply(clear_stopword, args=[stopword_file, postag, user_file])\n",
    "tweets[\"pkuseg_stop\"] = tweets[\"pkuseg\"].apply(clear_stopword, args=[stopword_file, postag, user_file])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   微博正文         100 non-null    string\n",
      " 1   去噪           100 non-null    string\n",
      " 2   jieba        100 non-null    object\n",
      " 3   pkuseg       100 non-null    object\n",
      " 4   jieba_stop   100 non-null    object\n",
      " 5   pkuseg_stop  100 non-null    object\n",
      "dtypes: object(4), string(2)\n",
      "memory usage: 4.8+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets.to_excel(\"../02Data/jieba_pkuseg.xlsx\")\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试特殊符号的长度\n",
    "\n",
    "a = '⚠️'\n",
    "b = '🌪️'\n",
    "c = '🌬'\n",
    "len(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 分词+词性标注 \n",
    "\n",
    "\n",
    "[pkuseg: 一个多领域中文分词工具包](https://github.com/lancopku/pkuseg-python)\n",
    "\n",
    "[jieba: 结巴中文分词](https://github.com/fxsjy/jieba)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('我', 'r'), ('爱', 'v'), ('北京', 'ns'), ('天安门', 'ns')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jieba 词性标注（part-of-speech tagging）\n",
    "\n",
    "def jieba_postag(str, use_paddle=True):\n",
    "    \"\"\"\n",
    "    jieba分词:\n",
    "    str         : 待分词的文本\n",
    "    use_paddle  : 是否使用paddle模型\n",
    "    \"\"\"\n",
    "    import jieba.posseg as pseg\n",
    "    word_tag_ls = pseg.lcut(str, use_paddle=use_paddle)\n",
    "    \n",
    "    res = [] # 将词语与词性从pair对象转换为 元组，方便索引\n",
    "    for w_t in word_tag_ls:\n",
    "        w, t = w_t\n",
    "        res.append((w, t))\n",
    "    return res\n",
    "\n",
    "\n",
    "#! 测试\n",
    "res = jieba_postag('我爱北京天安门')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('我', 'r'), ('爱', 'v'), ('北京', 'ns'), ('天安门', 'ns')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pkuseg 词性标注（part-of-speech tagging）\n",
    "# pkuseg 分词 \n",
    "\n",
    "def pkuseg_postag(str, m_name=\"web\", u_dict=\"default\", p_tag=True):\n",
    "    \"\"\"\n",
    "    pkuseg分词:\n",
    "    str     :待分词的文本\n",
    "    m_name  : 选择使用的预训练模型\n",
    "    u_dict  : 用户自定义的分词设置用户词典。\n",
    "    p_tag   : 是否进行词性标准（是的话，需要在自定义的词典中，也要添加相应的词性 tab键隔开在一行\n",
    "    \"\"\"\n",
    "    # m_name= \"news\"\n",
    "    # m_name= \"mixed\"\n",
    "    # p_tag= True # 是否包含词性\n",
    "    seg = pkuseg.pkuseg(model_name=m_name, user_dict=u_dict, postag=p_tag)           # 以默认配置加载模型    \n",
    "    res = seg.cut(str)  # 进行分词\n",
    "   \n",
    "    return seg.cut(str)  # 进行分词\n",
    "\n",
    "#! 测试\n",
    "res = pkuseg_postag('我爱北京天安门')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【面对暴雨，请收好这份#避险生存指南#】#蜀黍带你涨姿势#近日来，河南两小时暴雨记录又被刷新！接连不断的罕见暴雨袭击多个城市，熟悉的交通工具都变得不再安全，该如何远离“看不见的危险”?面对暴雨该如何避险脱困?戳视频↓收下这份生存指南！小央视频的微博视频\n",
      "jieba:\n",
      "[('【', 'x'), ('面对', 'v'), ('暴雨', 'n'), ('，', 'x'), ('请', 'v'), ('收好', 'v'), ('这份', 'mq'), ('#', 'x'), ('避险', 'v'), ('生存', 'v'), ('指南', 'n'), ('#', 'x'), ('】', 'x'), ('#', 'x'), ('蜀黍', 'n'), ('带', 'v'), ('你', 'r'), ('涨', 'v'), ('姿势', 'n'), ('#', 'x'), ('近日来', 'l'), ('，', 'x'), ('河南', 'ns'), ('两', 'm'), ('小时', 'n'), ('暴雨', 'n'), ('记录', 'n'), ('又', 'd'), ('被', 'p'), ('刷新', 'v'), ('！', 'x'), ('接连不断', 'l'), ('的', 'uj'), ('罕见', 'a'), ('暴雨', 'n'), ('袭击', 'v'), ('多个', 'm'), ('城市', 'ns'), ('，', 'x'), ('熟悉', 'v'), ('的', 'uj'), ('交通工具', 'l'), ('都', 'd'), ('变得', 'v'), ('不再', 'd'), ('安全', 'an'), ('，', 'x'), ('该', 'r'), ('如何', 'r'), ('远离', 'v'), ('“', 'x'), ('看不见', 'v'), ('的', 'uj'), ('危险', 'an'), ('”', 'x'), ('?', 'x'), ('面对', 'v'), ('暴雨', 'n'), ('该', 'r'), ('如何', 'r'), ('避险', 'v'), ('脱困', 'v'), ('?', 'x'), ('戳', 'v'), ('视频', 'n'), ('↓', 'x'), ('收下', 'v'), ('这份', 'mq'), ('生存', 'v'), ('指南', 'n'), ('！', 'x'), ('小央', 'n'), ('视频', 'n'), ('的', 'uj'), ('微博', 'a'), ('视频', 'n')]\n",
      "pkuseg:\n",
      "[('【', 'nr'), ('面对', 'v'), ('暴雨', 'n'), ('，', 'w'), ('请', 'v'), ('收好', 'v'), ('这', 'r'), ('份', 'q'), ('#', 'n'), ('避险', 'v'), ('生存', 'vn'), ('指南', 'n'), ('#', 'v'), ('】', 'v'), ('#', 'n'), ('蜀黍', 'nr'), ('带', 'v'), ('你', 'r'), ('涨', 'v'), ('姿势', 'n'), ('#', 'v'), ('近日来', 'l'), ('，', 'w'), ('河南', 'ns'), ('两', 'm'), ('小时', 'n'), ('暴雨', 'n'), ('记录', 'v'), ('又', 'd'), ('被', 'p'), ('刷新', 'v'), ('！', 'w'), ('接连不断', 'l'), ('的', 'u'), ('罕见', 'a'), ('暴雨', 'n'), ('袭击', 'v'), ('多', 'm'), ('个', 'q'), ('城市', 'n'), ('，', 'w'), ('熟悉', 'v'), ('的', 'u'), ('交通', 'n'), ('工具', 'n'), ('都', 'd'), ('变', 'v'), ('得', 'u'), ('不', 'd'), ('再', 'd'), ('安全', 'a'), ('，', 'w'), ('该', 'v'), ('如何', 'r'), ('远离', 'v'), ('“', 'w'), ('看', 'v'), ('不见', 'v'), ('的', 'u'), ('危险', 'an'), ('”', 'w'), ('?', 'v'), ('面对', 'v'), ('暴雨', 'n'), ('该', 'v'), ('如何', 'r'), ('避险', 'v'), ('脱困', 'v'), ('?戳', 'v'), ('视频', 'n'), ('↓', 'v'), ('收下', 'v'), ('这', 'r'), ('份', 'q'), ('生存', 'vn'), ('指南', 'n'), ('！', 'w'), ('小央', 'nr'), ('视频', 'n'), ('的', 'u'), ('微博', 'a'), ('视频', 'n')]\n"
     ]
    }
   ],
   "source": [
    "#! 测试\n",
    "word_str = tweets[\"去噪\"][0]\n",
    "print(word_str)\n",
    "\n",
    "print(\"jieba:\")\n",
    "res = jieba_postag(word_str)\n",
    "print(res)\n",
    "\n",
    "print(\"pkuseg:\")\n",
    "res = pkuseg_postag(word_str)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   微博正文         100 non-null    string\n",
      " 1   去噪           100 non-null    string\n",
      " 2   jieba        100 non-null    object\n",
      " 3   pkuseg       100 non-null    object\n",
      " 4   jieba_stop   100 non-null    object\n",
      " 5   pkuseg_stop  100 non-null    object\n",
      "dtypes: object(4), string(2)\n",
      "memory usage: 4.8+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   微博正文           100 non-null    string\n",
      " 1   去噪             100 non-null    string\n",
      " 2   jieba          100 non-null    object\n",
      " 3   pkuseg         100 non-null    object\n",
      " 4   jieba_stop     100 non-null    object\n",
      " 5   pkuseg_stop    100 non-null    object\n",
      " 6   jieba_postag   100 non-null    object\n",
      " 7   pkuseg_postag  100 non-null    object\n",
      "dtypes: object(6), string(2)\n",
      "memory usage: 6.4+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets[\"jieba_postag\"] = tweets[\"去噪\"].apply(jieba_postag)\n",
    "tweets[\"pkuseg_postag\"] = tweets[\"去噪\"].apply(pkuseg_postag)\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['面对', '暴雨', '收好', '这份', '避险', '生存', '指南', '蜀黍', '姿势', '近日来', '小时', '暴雨', '记录', '刷新', '接连不断', '罕见', '暴雨', '袭击', '多个', '城市', '熟悉', '交通工具', '变得', '不再', '安全', '远离', '看不见', '危险', '面对', '暴雨', '避险', '脱困', '视频', '收下', '这份', '生存', '指南', '小央', '视频', '微博', '视频']\n"
     ]
    }
   ],
   "source": [
    "#! 测试 带有词性标准的去停用词\n",
    "word_ls = tweets[\"jieba_postag\"][0]\n",
    "stopword_file = \"../05stopwords/hit_stopwords.txt\"\n",
    "postag = True\n",
    "user_file = \"../05stopwords/user_stopwords.txt\"\n",
    "\n",
    "res = clear_stopword(word_ls, stopword_file, postag, user_file) # 去噪\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   微博正文                100 non-null    string\n",
      " 1   去噪                  100 non-null    string\n",
      " 2   jieba               100 non-null    object\n",
      " 3   pkuseg              100 non-null    object\n",
      " 4   jieba_stop          100 non-null    object\n",
      " 5   pkuseg_stop         100 non-null    object\n",
      " 6   jieba_postag        100 non-null    object\n",
      " 7   pkuseg_postag       100 non-null    object\n",
      " 8   jieba_postag_stop   100 non-null    object\n",
      " 9   pkuseg_postag_stop  100 non-null    object\n",
      "dtypes: object(8), string(2)\n",
      "memory usage: 7.9+ KB\n"
     ]
    }
   ],
   "source": [
    "stopword_file = \"../05stopwords/hit_stopwords.txt\"\n",
    "postag = True\n",
    "user_file = \"../05stopwords/user_stopwords.txt\"\n",
    "\n",
    "tweets[\"jieba_postag_stop\"] = tweets[\"jieba_postag\"].apply(clear_stopword, args=[stopword_file, postag, user_file])\n",
    "tweets[\"pkuseg_postag_stop\"] = tweets[\"pkuseg_postag\"].apply(clear_stopword, args=[stopword_file, postag, user_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   微博正文                100 non-null    string\n",
      " 1   去噪                  100 non-null    string\n",
      " 2   jieba               100 non-null    object\n",
      " 3   pkuseg              100 non-null    object\n",
      " 4   jieba_stop          100 non-null    object\n",
      " 5   pkuseg_stop         100 non-null    object\n",
      " 6   jieba_postag        100 non-null    object\n",
      " 7   pkuseg_postag       100 non-null    object\n",
      " 8   jieba_postag_stop   100 non-null    object\n",
      " 9   pkuseg_postag_stop  100 non-null    object\n",
      "dtypes: object(8), string(2)\n",
      "memory usage: 7.9+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets.to_excel(\"../02Data/jieba_pkuseg.xlsx\")\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 文本去重 【考虑发表的用户，**官方号**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 文本标记\n",
    "\n",
    "- 手动整理标记，模型输入数据\n",
    "\n",
    "- 主题建模\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 特征词选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "移动 共享 ， 共享 汽车 ， 共享 经济 ， 共享 单车\n",
      "财经 栏目 ， 财经 政策 ， 经济 政策 ， 共享 经济\n",
      "+++++使用 CountVectorizer\n",
      "  (0, 5)\t1\n",
      "  (0, 0)\t4\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 6)\t2\n",
      "  (1, 7)\t2\n",
      "  (1, 3)\t1\n",
      "  (1, 2)\t2\n",
      "['共享' '单车' '政策' '栏目' '汽车' '移动' '经济' '财经']\n",
      "[[4 1 0 0 1 1 1 0]\n",
      " [1 0 2 1 0 0 2 2]]\n",
      "   共享  单车  政策  栏目  汽车  移动  经济  财经\n",
      "0   4   1   0   0   1   1   1   0\n",
      "1   1   0   2   1   0   0   2   2\n",
      "+++++使用 TfidfVectorizer\n",
      "  (0, 1)\t0.2935323404273021\n",
      "  (0, 6)\t0.20885067778197156\n",
      "  (0, 4)\t0.2935323404273021\n",
      "  (0, 0)\t0.8354027111278862\n",
      "  (0, 5)\t0.2935323404273021\n",
      "  (1, 2)\t0.5889689090267317\n",
      "  (1, 3)\t0.29448445451336586\n",
      "  (1, 7)\t0.5889689090267317\n",
      "  (1, 6)\t0.41905622959186595\n",
      "  (1, 0)\t0.20952811479593297\n",
      "['共享' '单车' '政策' '栏目' '汽车' '移动' '经济' '财经']\n",
      "[[0.83540271 0.29353234 0.         0.         0.29353234 0.29353234\n",
      "  0.20885068 0.        ]\n",
      " [0.20952811 0.         0.58896891 0.29448445 0.         0.\n",
      "  0.41905623 0.58896891]]\n",
      "         共享        单车        政策        栏目        汽车        移动        经济  \\\n",
      "0  0.835403  0.293532  0.000000  0.000000  0.293532  0.293532  0.208851   \n",
      "1  0.209528  0.000000  0.588969  0.294484  0.000000  0.000000  0.419056   \n",
      "\n",
      "         财经  \n",
      "0  0.000000  \n",
      "1  0.588969  \n"
     ]
    }
   ],
   "source": [
    "# 测试学习\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from  sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba\n",
    "\n",
    "data=[\"移动共享，共享汽车，共享经济，共享单车\",\n",
    "     \"财经栏目，财经政策，经济政策，共享经济\"] \n",
    "\n",
    "# 需要提前分词\n",
    "\n",
    "# 分词\n",
    "cut_data=[]\n",
    "for s in data:\n",
    "    cut_s = jieba.cut(s)\n",
    "    l_cut_s=' '.join(list(cut_s))    \n",
    "    cut_data.append(l_cut_s)\n",
    "    print(l_cut_s)\n",
    "    \n",
    "# 使用 CountVectorizer\n",
    "print(\"+++++使用 CountVectorizer\")\n",
    "transfer = CountVectorizer(stop_words=[\"打算\",\"就是\"]) \n",
    "#实例化一个转换器类,\n",
    "# # stop_words=[\"打算\",\"就是\"],去除不想要的词\n",
    "data_new = transfer.fit_transform(cut_data)  #调用fit_transform()\n",
    "print(data_new)\n",
    "print(transfer.get_feature_names_out())\n",
    "print(data_new.toarray()) \n",
    "#构建成一个二维表：\n",
    "df=pd.DataFrame(data_new.toarray(),columns=transfer.get_feature_names_out())\n",
    "print(df)   \n",
    "\n",
    "# 使用 TfidfVectorizer\n",
    "print(\"+++++使用 TfidfVectorizer\")\n",
    "transfer = TfidfVectorizer() #实例化一个转换器类\n",
    "data_new = transfer.fit_transform(cut_data) #调用fit_transform()\n",
    "print(data_new)\n",
    "print(transfer.get_feature_names_out())\n",
    "print(data_new.toarray()) \n",
    "#构建成一个二维表：\n",
    "df=pd.DataFrame(data_new.toarray(),columns=transfer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用封装\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from  sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def word2vec(lsls, v_type):\n",
    "    ls_str=[]\n",
    "    for s in lsls:\n",
    "        strs = ' '.join(s) \n",
    "        ls_str.append(strs)\n",
    "    \n",
    "    if v_type == \"TFIDF\":\n",
    "    # TF-IDF(term frequency—inverse document frequency)\n",
    "        transfer = TfidfVectorizer() #实例化一个转换器类\n",
    "    else:\n",
    "        transfer = CountVectorizer()\n",
    "    data_new = transfer.fit_transform(ls_str) #调用fit_transform()\n",
    "    #构建成一个二维表：\n",
    "    df=pd.DataFrame(data_new.toarray(), columns=transfer.get_feature_names_out())\n",
    "    \n",
    "    withWeight = transfer.vocabulary_\n",
    "    return df, withWeight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e481c8094450a362835654784466fc3605d9d17b45d80f6ff62525ad7933ac54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
