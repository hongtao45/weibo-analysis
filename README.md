# weibo-tweets-analysis

## 文本处理流程

> 代码：
>
> test.ipynb
>
> 参考文献：
>
> - 基于微博数据的高速公路交通事件研究_包丹
> - 基于社交网络数据的交通突发事件识别方法_刘昭



- [ ] 流程梳理

1. 随机选取200条，用来做数据预处理的数量即可`df.smaple()`
2. 文本预处理
   1. 文本去噪
   2. 中文分词
   3. 过滤停用词
   4. HanLP 词性标注 【没有特殊需求，咱们就不用做了
   5. 文本去重 【考虑发表的用户，**官方号**
   6. 文本标记【手动整理模型输入数据
   7. 特征词选择

3. 数据向量化，分类器模型可以直接使用



- [ ]  训练样本处理

- 对于训练集，自然语言处理过程包括：
  - 中文分词、过滤停用词、特征权重计算、特征词选取
- 而对于测试集：
  - 特征权重计算、特征词



## 具体代码实现

### 1.文本去噪

> 微博中的特殊符号，对于算法的训练来说不太有利
>
> [Python正则表达式清洗微博文本特殊符号(网址, @, 表情符等)](https://blog.csdn.net/blmoistawinde/article/details/103648044)



- 微博里主要有几种特殊格式：
  1. 网页
  2. @用户名（包括转发路径上的其他用户名）
  3. 表情符号(用[]包围)
  4. 话题 “#topic#”(用#包围)
  4. 分类标签“【content】”
  4. 删除字符数小于10 的过短微博
- 分析：
  - 1：没有用
  - 2：有时候会@官方号啥的，部分会有用，再考虑
  - 3：表情包里面也会有一些表情信息，[哈哈] ，麻烦
  - 4：这个就很重要的了，保留
  - 5：标签里面，也有一些重要信息的，再考虑
- 疑问：
  - 话题两边的“#”要不要删掉，影响分词
  - 现在的代码“@”删的不彻底，还是会留下一些
  - 应用到DataFrame的每一个数据上map()函数
  - 



### 2. 中文分词

> [结巴中文分词](https://github.com/fxsjy/jieba)

- 示例分析
  - 现在示例代码/文字只是依据单独的话，没有标点符号
  - 中英文的标点符号，也会被分词 - 后面过滤停用词的时候再删掉就好
  - 应用到DataFrame的每一个数据上map()函数
  - 

### 3.过滤停用词

> [Python-中文分词并去除停用词仅保留汉字](https://blog.csdn.net/lztttao/article/details/104723228)
>
> 

