{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习如何进行文本处理\n",
    "> 对于训练集，自然语言处理过程包括：\n",
    "> \n",
    ">     中文分词、过滤停用词、特征权重计算、特征词选取。\n",
    "> \n",
    "> 而对于测试集：\n",
    "> \n",
    ">     特征权重计算、特征词\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用的包 导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import matplotlib.pyplot as plt\n",
    "from  sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原始数据及基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bid</th>\n",
       "      <th>user_id</th>\n",
       "      <th>用户昵称</th>\n",
       "      <th>微博正文</th>\n",
       "      <th>头条文章url</th>\n",
       "      <th>发布位置</th>\n",
       "      <th>艾特用户</th>\n",
       "      <th>话题</th>\n",
       "      <th>转发数</th>\n",
       "      <th>评论数</th>\n",
       "      <th>点赞数</th>\n",
       "      <th>发布时间</th>\n",
       "      <th>发布工具</th>\n",
       "      <th>微博图片url</th>\n",
       "      <th>微博视频url</th>\n",
       "      <th>retweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4632157211069133</td>\n",
       "      <td>Kduyt4u85</td>\n",
       "      <td>7073634525</td>\n",
       "      <td>中国交通</td>\n",
       "      <td>#我家门口那条路#5月1日20时至2日20时，新疆天山西部、西藏中部等地部分地方有雨夹雪。西...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>我家门口那条路,国家综合立体交通网规划纲要,中国路网</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2021-05-01 21:30</td>\n",
       "      <td>微博 weibo.com</td>\n",
       "      <td>['https://wx3.sinaimg.cn/large/007IIftrly1gq34...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4632138676964878</td>\n",
       "      <td>Kdu4ztdSK</td>\n",
       "      <td>6360489687</td>\n",
       "      <td>昌江交警</td>\n",
       "      <td>#五一我在岗##畅行中国,交警同行#五一，大雨滂沱，而你无阻。你是烈日暴雨下的指示牌，是人民...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>海南交警,中国交通频道,公安部交通管理局</td>\n",
       "      <td>五一我在岗,畅行中国,交警同行</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-05-01 20:16</td>\n",
       "      <td>新版微博 weibo.com</td>\n",
       "      <td>['https://wx2.sinaimg.cn/large/006WrXVlgy1gq36...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4632091960806995</td>\n",
       "      <td>KdsRe3nW3</td>\n",
       "      <td>2098013967</td>\n",
       "      <td>高州天气</td>\n",
       "      <td>【高州天气预报】今晚到明天白天，多云，有（雷）阵雨局部大雨，气温22-29℃，相对湿度65%...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-05-01 17:10</td>\n",
       "      <td>微博 weibo.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4632061941647124</td>\n",
       "      <td>Kds4O6Uuw</td>\n",
       "      <td>5323286636</td>\n",
       "      <td>广德公安在线</td>\n",
       "      <td>#我在岗位上#【狂风暴雨中交警保畅通】4月30日晚，狂风暴雨突然袭来。@广德交警面对恶劣天气...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>广德交警</td>\n",
       "      <td>我在岗位上,我为群众办实事</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-05-01 15:11</td>\n",
       "      <td>360安全浏览器</td>\n",
       "      <td>['https://wx1.sinaimg.cn/large/005OfY1Sly1gq2y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4632029499493949</td>\n",
       "      <td>KdretDPOd</td>\n",
       "      <td>2578964252</td>\n",
       "      <td>广德交警</td>\n",
       "      <td>#我为群众办实事#五一前夕，暴雨来袭！广德公安交警在岗在位，加强队所联勤，全力疏导交通、清理...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>公安部交通管理局,安徽公安交警在线,广德公安在线,宣城公安交警在线</td>\n",
       "      <td>我为群众办实事</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-05-01 13:02</td>\n",
       "      <td>荣耀20 PRO</td>\n",
       "      <td>['https://wx2.sinaimg.cn/large/99b7df1cly1gq2u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id        bid     user_id    用户昵称  \\\n",
       "0  4632157211069133  Kduyt4u85  7073634525    中国交通   \n",
       "1  4632138676964878  Kdu4ztdSK  6360489687    昌江交警   \n",
       "2  4632091960806995  KdsRe3nW3  2098013967    高州天气   \n",
       "3  4632061941647124  Kds4O6Uuw  5323286636  广德公安在线   \n",
       "4  4632029499493949  KdretDPOd  2578964252    广德交警   \n",
       "\n",
       "                                                微博正文 头条文章url 发布位置  \\\n",
       "0  #我家门口那条路#5月1日20时至2日20时，新疆天山西部、西藏中部等地部分地方有雨夹雪。西...     NaN  NaN   \n",
       "1  #五一我在岗##畅行中国,交警同行#五一，大雨滂沱，而你无阻。你是烈日暴雨下的指示牌，是人民...     NaN  NaN   \n",
       "2  【高州天气预报】今晚到明天白天，多云，有（雷）阵雨局部大雨，气温22-29℃，相对湿度65%...     NaN  NaN   \n",
       "3  #我在岗位上#【狂风暴雨中交警保畅通】4月30日晚，狂风暴雨突然袭来。@广德交警面对恶劣天气...     NaN  NaN   \n",
       "4  #我为群众办实事#五一前夕，暴雨来袭！广德公安交警在岗在位，加强队所联勤，全力疏导交通、清理...     NaN  NaN   \n",
       "\n",
       "                                艾特用户                          话题  转发数  评论数  \\\n",
       "0                                NaN  我家门口那条路,国家综合立体交通网规划纲要,中国路网    1    3   \n",
       "1               海南交警,中国交通频道,公安部交通管理局             五一我在岗,畅行中国,交警同行    0    0   \n",
       "2                                NaN                         NaN    0    0   \n",
       "3                               广德交警               我在岗位上,我为群众办实事    0    0   \n",
       "4  公安部交通管理局,安徽公安交警在线,广德公安在线,宣城公安交警在线                     我为群众办实事    0    0   \n",
       "\n",
       "   点赞数              发布时间            发布工具  \\\n",
       "0    6  2021-05-01 21:30    微博 weibo.com   \n",
       "1    0  2021-05-01 20:16  新版微博 weibo.com   \n",
       "2    0  2021-05-01 17:10    微博 weibo.com   \n",
       "3    1  2021-05-01 15:11        360安全浏览器   \n",
       "4    0  2021-05-01 13:02        荣耀20 PRO   \n",
       "\n",
       "                                             微博图片url 微博视频url  retweet_id  \n",
       "0  ['https://wx3.sinaimg.cn/large/007IIftrly1gq34...     NaN         NaN  \n",
       "1  ['https://wx2.sinaimg.cn/large/006WrXVlgy1gq36...     NaN         NaN  \n",
       "2                                                NaN     NaN         NaN  \n",
       "3  ['https://wx1.sinaimg.cn/large/005OfY1Sly1gq2y...     NaN         NaN  \n",
       "4  ['https://wx2.sinaimg.cn/large/99b7df1cly1gq2u...     NaN         NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Data/完整数据_暴雨_交通.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15155, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15155 entries, 0 to 15154\n",
      "Data columns (total 17 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          15155 non-null  int64  \n",
      " 1   bid         15155 non-null  object \n",
      " 2   user_id     15155 non-null  int64  \n",
      " 3   用户昵称        15155 non-null  object \n",
      " 4   微博正文        15155 non-null  object \n",
      " 5   头条文章url     21 non-null     object \n",
      " 6   发布位置        1127 non-null   object \n",
      " 7   艾特用户        4092 non-null   object \n",
      " 8   话题          10761 non-null  object \n",
      " 9   转发数         15155 non-null  int64  \n",
      " 10  评论数         15155 non-null  int64  \n",
      " 11  点赞数         15155 non-null  int64  \n",
      " 12  发布时间        15155 non-null  object \n",
      " 13  发布工具        14913 non-null  object \n",
      " 14  微博图片url     6400 non-null   object \n",
      " 15  微博视频url     5130 non-null   object \n",
      " 16  retweet_id  0 non-null      float64\n",
      "dtypes: float64(1), int64(5), object(11)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                0\n",
       "bid               0\n",
       "user_id           0\n",
       "用户昵称              0\n",
       "微博正文              0\n",
       "头条文章url       15134\n",
       "发布位置          14028\n",
       "艾特用户          11063\n",
       "话题             4394\n",
       "转发数               0\n",
       "评论数               0\n",
       "点赞数               0\n",
       "发布时间              0\n",
       "发布工具            242\n",
       "微博图片url        8755\n",
       "微博视频url       10025\n",
       "retweet_id    15155\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机的200行，测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 17)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.sample(n=200, replace=False, random_state=1)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6826     【面对暴雨，请收好这份#避险生存指南#】#蜀黍带你涨姿势#近日来，河南两小时暴雨记录又被刷新...\n",
       "1432     #河南高速路况#截至2021年9月9日,19:00，目前省内高速通行情况：1、因收费站临时抢...\n",
       "6965     郑州大暴雨灾害这波，今天回来又看到一交通事故，一个大叔满身血淋淋坐在车前，前几日杭州电瓶车骑...\n",
       "12476    【暖心！#交警暴雨中执勤频频被路人送伞#】6月10日17时许，浙江温州泰顺县一路口严重拥堵，...\n",
       "3023     【湖南多地迎暴雨！这些收费站仍在管制中】今日晚间，湖南部分地区突降暴雨，905中国交通广播派...\n",
       "                               ...                        \n",
       "3014     暴雨来袭，行人户外活动谨防雷电！注意安全！转发，提醒更多人↓↓↓#长沙打雷##交通安全不放假...\n",
       "6469     #河南高速路况#截至2021年7月29日11:05，1、因积水，禁止上下站的收费站有：菏宝高...\n",
       "13372    #大众观天下#【暖心！#交警暴雨中执勤频频被路人送伞#】6月10日17时许，浙江温州泰顺县一...\n",
       "11309    珠海交通【突发：受暴雨影响，梅界路交蓝盾路路口暂时禁止通行！】受暴雨影响，梅界路交蓝盾路奥园...\n",
       "5623     【#外卖小哥雷电暴雨中指挥交通#：能帮大家尽快回家挺开心】据@看看新闻KNEWS消息：近日，...\n",
       "Name: 微博正文, Length: 200, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['微博正文']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 文本去噪\n",
    "去除一些无用的信息；特殊的符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_noise(text):\n",
    "    text = text.replace(u'\\xa0', u' ')      # 去除 \\xa0     不间断空白符 \n",
    "    text = text.replace(u'\\u3000', u' ')    # 去除 \\u3000   全角的空白符\n",
    "    \n",
    "    text = re.sub(r\"(回复)?(//)?\\s*@\\S*?\\s*(:| |$)\", \"\", text)  # 去除正文中的@和回复/转发中的用户名\n",
    "    text = re.sub(r\"\\[\\S+\\]\", \"\", text)     # 去除表情符号\n",
    "    # text = re.sub(r\"#\\S+#\", \"\", text)     # 去除话题内容\n",
    "    URL_REGEX = re.compile(\n",
    "        r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
    "        re.IGNORECASE)\n",
    "    text = re.sub(URL_REGEX, \"\", text)      # 去除网址\n",
    "    \n",
    "    EMAIL_REGEX = re.compile(r\"[-a-z0-9_.]+@(?:[-a-z0-9]+\\.)+[a-z]{2,6}\", re.IGNORECASE)\n",
    "    text = re.sub(EMAIL_REGEX, \"\", text)    # 去除邮件 \n",
    "    \n",
    "    text = text.replace(\"转发微博\", \"\")      # 去除无意义的词语\n",
    "    text = text.replace(\"网页链接\", \"\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)        # 合并正文中过多的空格\n",
    "\n",
    "    text = re.sub(r\"\\d{2,4}年|\\d{1,2}月|\\d{1,2}日|\\d{1,2}时|\\d{1,2}分| \\d{1,2}点\", \"\", text) # 去除 日期 时间\n",
    "    text = re.sub(r\"\\d\", \"\", text)\n",
    "    \n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试  和改进 clean_noise() 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "太过分了招行最近负面新闻越来越多呀...\n",
      "希望你?得好?我本＂肥血?史＂\n",
      "杨大哥\n",
      "【#赵薇#：正筹备下一部电影 但不是青春片....\n",
      "哈哈 试了我家那位，说我属于＂放心的没法看的＂\n"
     ]
    }
   ],
   "source": [
    "print(clean_noise(\"太过分了@Rexzhenghao //@Janie_Zhang:招行最近负面新闻越来越多呀...1234\"))\n",
    "print(clean_noise(\"希望你?得好?我本＂@Pete三姑父\\xa0肥血?史＂[晕][哈哈]@Pete三姑父\"))\n",
    "print(clean_noise(\"回复@钱旭明QXM:[嘻嘻][嘻嘻] //@钱旭明QXM:杨大哥[good][good]\"))\n",
    "print(clean_noise(\"【#赵薇#：正筹备下一部电影 但不是青春片....http://t.cn/8FLopdQ\"))\n",
    "print(clean_noise(\"[酷]//@芊如_GZ:[哈哈]// @布丁clout : 哈哈 // @audrey-panda :试了我家那位，说我属于＂放心的没法看的＂[晕] // @芊如_GZ :[晕]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这', '是测试', '用例']\n",
      "这,!是测试用例\n",
      "这是测试用例\n",
      "测试时间是16时47分\n",
      "测试时间是2021年16时47分\n",
      "测试时间是21年05月30日\n",
      "测试时间是年月日时分\n",
      "测试时间是\n",
      "测试时间是年月日时分\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'[\\u4e00-\\u9fa5]+', '这,!是测试12用例')) #匹配汉字\n",
    "print(re.sub(\"\\d\", \"\", '这,!是测试12用例'))\n",
    "print(''.join(re.findall('[\\u4e00-\\u9fa5]', '这,!是112测$$试12用例')))\n",
    "\n",
    "print(re.sub(r\"(\\d{2,4}年\\d{1,2}月\\d{1,2}日)\",\"\",\"测试时间是2021年05月30日16时47分\")) # 去除日期，时间\n",
    "print(re.sub(r\"(\\d{1,2}月\\d{1,2}日)\", \"\", \"测试时间是2021年05月30日16时47分\")) # 去除日期 \n",
    "print(re.sub(r\"(\\d{1,2}时\\d{1,2}分)\",\"\",\"测试时间是21年05月30日16时47分\")) # 去除时间\n",
    "print(re.sub(\"\\d\", \"\",\"测试时间是21年05月30日16时47分\" ))\n",
    "print(re.sub(r\"\\d{2,4}年|\\d{1,2}月|\\d{1,2}日|\\d{1,2}时|\\d{1,2}分\",\"\",\"测试时间是2021年30日16时47分\"))\n",
    "\n",
    "print(re.sub(r\"\\d\", \"\", \"测试时间是21年05月30日16时47分\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发布了头条文章：《【交管前沿（34）】暴雨期间，救助出行群众于危急之中》河南交警平安鹤壁O【交管前沿（34）】暴雨期间，救助出行群众于危急之中\n",
      "发布了头条文章：《【交管前沿（34）】暴雨期间，救助出行群众于危急之中》\n",
      "发布了头条文章：《【交管前沿（）】暴雨期间，救助出行群众于危急之中》\n"
     ]
    }
   ],
   "source": [
    "txt =\"发布了头条文章：《【交管前沿（34）】暴雨期间，救助出行群众于危急之中》@公安部交通管理局@河南交警@平安中原@平安鹤壁O【交管前沿（34）】暴雨期间，救助出行群众于危急之中\"\n",
    "# txt =\"#实时路况信息#游仙区绵梓路魏城段现目前暴雨骤至，能见度低，请过往车辆保持车距，注意行车安全。@中国交通频道·四川@平安绵阳@绵阳公安交警@微游仙L绵阳交警支队直属三大队的微博视频#压事故、整秩序、保平安#\"\n",
    "print(re.sub(r\"@\\S*?\\s*(@|:| |$)\",\"\",txt))\n",
    "print(re.sub(r\"@\\S*?\\s*(:| |$)\",\"\",txt))\n",
    "print(clean_noise(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'希望你?得好?我本＂@Pete三姑父\\xa0肥血?史＂[晕][哈哈]@Pete三姑父'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tmp = \"希望你?得好?我本＂@Pete三姑父\\xa0肥血?史＂[晕][哈哈]@Pete三姑父\"\n",
    "text_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'希望你?得好?我本＂@Pete三姑父 肥血?史＂[晕][哈哈]@Pete三姑父'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tmp.replace(u'\\xa0', u' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'【雨中，你们坚守的样子真帅！】6月17日早上8点，正值出行早高峰，一场突袭的暴雨倾注了这座城市，为确保雨天辖区道路安全畅通，避免因大雨造成的交通事故，@南充交警一大队\\xa0全体民辅警，冒雨在辖区各个主要路段执勤，有的甚至来不及换上雨衣，淋着雨指挥车辆，疏导交通，全力保障恶劣天气下市民出行畅通，成为大雨中最美丽的风景线。@四川公安@四川交警@四川长安网L南充公安的微博视频'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = df.loc[:, ['微博正文']]\n",
    "tmp = tweets.iloc[6, 0]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'【雨中，你们坚守的样子真帅！】早上点，正值出行早高峰，一场突袭的暴雨倾注了这座城市，为确保雨天辖区道路安全畅通，避免因大雨造成的交通事故，全体民辅警，冒雨在辖区各个主要路段执勤，有的甚至来不及换上雨衣，淋着雨指挥车辆，疏导交通，全力保障恶劣天气下市民出行畅通，成为大雨中最美丽的风景线。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1 = clean_noise(tmp)\n",
    "res1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 中文分词\n",
    "\n",
    "这里只是学习，分词和停用词过滤后面一起实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paddle enabled successfully......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paddle Mode: 我/来到/北京清华大学\n",
      "Paddle Mode: 乒乓球/拍卖/完/了\n",
      "Paddle Mode: 中国科学技术大学\n",
      "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学/ 【】\n",
      "Default Mode: 我/ 来到/ 北京/ 清华大学/ 【/ 】\n",
      "他, 来到, 了, 网易, 杭研, 大厦\n",
      "小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n"
     ]
    }
   ],
   "source": [
    "\n",
    "jieba.enable_paddle()# 启动paddle模式。 0.40版之后开始支持，早期版本不支持\n",
    "strs=[\"我来到北京清华大学\",\"乒乓球拍卖完了\",\"中国科学技术大学\"]\n",
    "for str in strs:\n",
    "    seg_list = jieba.cut(str, use_paddle=True) # 使用paddle模式\n",
    "    print(\"Paddle Mode: \" + '/'.join(list(seg_list)))\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学【】\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
    "\n",
    "seg_list = jieba.cut(\"我来到北京清华大学【】\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\")  # 默认是精确模式\n",
    "print(\", \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  # 搜索引擎模式\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '来到', '北京', '清华', '清华大学', '华大', '大学', '【】']\n"
     ]
    }
   ],
   "source": [
    "# jieba.lcut() 直接返回列表\n",
    "seg_list = jieba.lcut(\"我来到北京清华大学【】\", cut_all=True)\n",
    "# print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
    "print(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入词典\n",
    "# 用法： jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径\n",
    "# 词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。\n",
    "# 词频省略时使用自动计算的能保证分出该词的词频。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['疯狂', '囤金', '，', '中国大妈', '震惊', '华尔街']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '疯狂囤金，中国大妈震惊华尔街'\n",
    "jieba.load_userdict('CiDian.txt')\n",
    "wordlist = jieba.lcut(text)\n",
    "wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 过滤停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取停用词列表\n",
    "def get_stopword_list(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:    # \n",
    "        stopword_list = [word.strip('\\n') for word in f.readlines()]\n",
    "    return stopword_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词 然后清除停用词语\n",
    "def clean_stopword(str, stopword_list):\n",
    "    result = []\n",
    "    word_list = jieba.cut(str)   # 分词后返回一个列表  jieba.cut(）   返回的是一个迭代器\n",
    "    for w in word_list:\n",
    "        if w not in stopword_list:\n",
    "            result.append(w)\n",
    "   \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python 判断字符串的内容是不是数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:  # 如果能运行float(s)语句，返回True（字符串s是浮点数）\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:  # ValueError为Python的一种标准异常，表示\"传入无效的参数\"\n",
    "        pass  # 如果引发了ValueError这种异常，不做任何事情（pass：不做任何事情，一般用做占位语句）\n",
    "    try:\n",
    "        import unicodedata  # 处理ASCii码的包\n",
    "        unicodedata.numeric(s)  # 把一个表示数字的字符串转换为浮点数返回的函数\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "str1 = '1'\n",
    "print(is_number(str1))\n",
    "str2 = '1.1'\n",
    "print(is_number(str2))\n",
    "str3 = '-1'\n",
    "print(is_number(str3))\n",
    "str4 = '我们'\n",
    "print(is_number(str4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['雨中', '坚守', '样子', '真帅', '早上', '点', '正值', '出行', '早', '高峰', '一场', '突袭', '暴雨', '倾注', '这座', '城市', '确保', '雨天', '辖区', '道路', '安全', '畅通', '避免', '大雨', '造成', '交通事故', '全体', '民辅警', '冒雨', '辖区', '主要', '路段', '执勤', '来不及', '换上', '雨衣', '淋着', '雨', '指挥', '车辆', '疏导', '交通', '全力', '保障', '恶劣', '天气', '下', '市民', '出行', '畅通', '成为', '大雨', '中', '最', '美丽', '风景线']\n"
     ]
    }
   ],
   "source": [
    "stopword_list = get_stopword_list('stopwords/hit_stopwords.txt')\n",
    "\n",
    "res23 = clean_stopword(res1, stopword_list)\n",
    "res23= [w for w in res23]\n",
    "print(res23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(map(is_number, res23)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.词性标注 \n",
    "没有这个需求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.文本去重 \n",
    "\n",
    "统一处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.8421052631578947\n",
      "0.5454545454545454\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    " \n",
    "def string_similar(s1, s2):\n",
    "    return difflib.SequenceMatcher(None, s1, s2).quick_ratio()\n",
    "\n",
    "\n",
    "print(string_similar('爱尔眼科沪滨医院', '沪滨爱尔眼科医院'))\n",
    "print(string_similar('安定区妇幼保健站', '定西市安定区妇幼保健站'))\n",
    "print(string_similar('广州市医院', '广东省中医院'))\n",
    "print(string_similar(\"广州市医院 - 我和你\",\"你和我 - 广州市医院\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.文本标记\n",
    "\n",
    "手动标记"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.特征词选择\n",
    "\n",
    "统一处理和学习，这个就比较麻烦了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('大雨', 0.3348499425784),\n",
       " ('畅通', 0.3313562621724),\n",
       " ('辖区', 0.32669345500319996),\n",
       " ('出行', 0.3154784998372),\n",
       " ('真帅', 0.23909535005799998),\n",
       " ('民辅警', 0.23909535005799998),\n",
       " ('淋着', 0.23909535005799998),\n",
       " ('雨中', 0.208698834984),\n",
       " ('风景线', 0.20579519478600003),\n",
       " ('冒雨', 0.20144072511)]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = \"\".join(res23) #\n",
    "topK = 10\n",
    "tags = jieba.analyse.extract_tags(content, topK=topK, withWeight=True)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英文特征提取 sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I have a dog.\",\n",
    "    \"You have a dog and a cat.\",\n",
    "    \"He books a book.\",\n",
    "    \"No cost too great.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  book  books  cat  cost  dog  great  have  he  no  too  you\n",
      "0    0     0      0    0     0    1      0     1   0   0    0    0\n",
      "1    1     0      0    1     0    1      0     1   0   0    0    1\n",
      "2    0     1      1    0     0    0      0     0   1   0    0    0\n",
      "3    0     0      0    0     1    0      1     0   0   1    1    0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "counter = CountVectorizer() # 先使用默认参数\n",
    "counter.fit(corpus)\n",
    "X = counter.transform(corpus)\n",
    "\n",
    "df=pd.DataFrame(X.toarray(),columns=counter.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'have': 7, 'dog': 5, 'you': 11, 'and': 0, 'cat': 3, 'he': 8, 'books': 2, 'book': 1, 'no': 9, 'cost': 4, 'too': 10, 'great': 6}\n"
     ]
    }
   ],
   "source": [
    "print(counter.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [1 0 0 1 0 1 0 1 0 0 0 1]\n",
      " [0 1 1 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 1 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.todense()) # X是一个稀疏矩阵，输出稠密化表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        and     book    books       cat  cost       dog  great      have  \\\n",
      "0  0.000000  0.00000  0.00000  0.000000   0.0  0.707107    0.0  0.707107   \n",
      "1  0.485461  0.00000  0.00000  0.485461   0.0  0.382743    0.0  0.382743   \n",
      "2  0.000000  0.57735  0.57735  0.000000   0.0  0.000000    0.0  0.000000   \n",
      "3  0.000000  0.00000  0.00000  0.000000   0.5  0.000000    0.5  0.000000   \n",
      "\n",
      "        he   no  too       you  \n",
      "0  0.00000  0.0  0.0  0.000000  \n",
      "1  0.00000  0.0  0.0  0.485461  \n",
      "2  0.57735  0.0  0.0  0.000000  \n",
      "3  0.00000  0.5  0.5  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer() # 先使用默认参数\n",
    "tfidf.fit(corpus)\n",
    "X = tfidf.transform(corpus)\n",
    "\n",
    "df=pd.DataFrame(X.toarray(),columns=tfidf.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'have': 7, 'dog': 5, 'you': 11, 'and': 0, 'cat': 3, 'he': 8, 'books': 2, 'book': 1, 'no': 9, 'cost': 4, 'too': 10, 'great': 6}\n",
      "['and' 'book' 'books' 'cat' 'cost' 'dog' 'great' 'have' 'he' 'no' 'too'\n",
      " 'you']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.vocabulary_)\n",
    "print(tfidf.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.70710678\n",
      "  0.         0.70710678 0.         0.         0.         0.        ]\n",
      " [0.48546061 0.         0.         0.48546061 0.         0.38274272\n",
      "  0.         0.38274272 0.         0.         0.         0.48546061]\n",
      " [0.         0.57735027 0.57735027 0.         0.         0.\n",
      "  0.         0.         0.57735027 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.5        0.\n",
      "  0.5        0.         0.         0.5        0.5        0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文特征提取 sklearn + jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "移动 共享 ， 共享 汽车 ， 共享 经济 ， 共享 单车\n",
      "财经 栏目 ， 财经 政策 ， 经济 政策 ， 共享 经济\n",
      "['移动 共享 ， 共享 汽车 ， 共享 经济 ， 共享 单车', '财经 栏目 ， 财经 政策 ， 经济 政策 ， 共享 经济']\n",
      "  (0, 1)\t0.2935323404273021\n",
      "  (0, 6)\t0.20885067778197156\n",
      "  (0, 4)\t0.2935323404273021\n",
      "  (0, 0)\t0.8354027111278862\n",
      "  (0, 5)\t0.2935323404273021\n",
      "  (1, 2)\t0.5889689090267317\n",
      "  (1, 3)\t0.29448445451336586\n",
      "  (1, 7)\t0.5889689090267317\n",
      "  (1, 6)\t0.41905622959186595\n",
      "  (1, 0)\t0.20952811479593297\n",
      "  (0, 1)\t0.2935323404273021\n",
      "  (0, 6)\t0.20885067778197156\n",
      "  (0, 4)\t0.2935323404273021\n",
      "  (0, 0)\t0.8354027111278862\n",
      "  (0, 5)\t0.2935323404273021\n",
      "  (1, 2)\t0.5889689090267317\n",
      "  (1, 3)\t0.29448445451336586\n",
      "  (1, 7)\t0.5889689090267317\n",
      "  (1, 6)\t0.41905622959186595\n",
      "  (1, 0)\t0.20952811479593297\n",
      "['共享' '单车' '政策' '栏目' '汽车' '移动' '经济' '财经']\n",
      "[[0.83540271 0.29353234 0.         0.         0.29353234 0.29353234\n",
      "  0.20885068 0.        ]\n",
      " [0.20952811 0.         0.58896891 0.29448445 0.         0.\n",
      "  0.41905623 0.58896891]]\n",
      "         共享        单车        政策        栏目        汽车        移动        经济  \\\n",
      "0  0.835403  0.293532  0.000000  0.000000  0.293532  0.293532  0.208851   \n",
      "1  0.209528  0.000000  0.588969  0.294484  0.000000  0.000000  0.419056   \n",
      "\n",
      "         财经  \n",
      "0  0.000000  \n",
      "1  0.588969  \n"
     ]
    }
   ],
   "source": [
    "#数据\n",
    "data=[\"移动共享，共享汽车，共享经济，共享单车\",\n",
    "     \"财经栏目，财经政策，经济政策，共享经济\"] \n",
    "\n",
    "# 分词\n",
    "cut_data=[]\n",
    "for s in data:\n",
    "    cut_s = jieba.cut(s)\n",
    "    l_cut_s=' '.join(list(cut_s))    \n",
    "    cut_data.append(l_cut_s)\n",
    "    print(l_cut_s)\n",
    "\n",
    "print(cut_data)\n",
    "\n",
    "# TF-IDF\n",
    "transfer = TfidfVectorizer() #实例化一个转换器类\n",
    "data_new = transfer.fit_transform(cut_data) #调用fit_transform()\n",
    "print(data_new)\n",
    "print(data_new)\n",
    "print(transfer.get_feature_names_out())\n",
    "print(data_new.toarray()) \n",
    "#构建成一个二维表：\n",
    "df=pd.DataFrame(data_new.toarray(),columns=transfer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29353234, 0.20885068, 0.29353234, 0.83540271, 0.29353234,\n",
       "       0.58896891, 0.29448445, 0.58896891, 0.41905623, 0.20952811])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今年 国庆节 打算 去 海南岛 度假', '享受 生活 ， 顺其自然 。 这 就是 生活 !']\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 5)\t2\n",
      "  (1, 6)\t1\n",
      "['享受' '今年' '国庆节' '度假' '海南岛' '生活' '顺其自然']\n",
      "[[0 1 1 1 1 0 0]\n",
      " [1 0 0 0 0 2 1]]\n",
      "   享受  今年  国庆节  度假  海南岛  生活  顺其自然\n",
      "0   0   1    1   1    1   0     0\n",
      "1   1   0    0   0    0   2     1\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data=[u'今年国庆节打算去海南岛度假',\"享受生活，顺其自然。这就是生活!\"]\n",
    "\n",
    "#分词\n",
    "cut_data=[]\n",
    "for s in data:\n",
    "    cut_s=jieba.cut(s)    \n",
    "    l_cut_s=' '.join(list(cut_s))    \n",
    "    cut_data.append(l_cut_s)\n",
    "\n",
    "print(cut_data) \n",
    "#统计特征词出现次数\n",
    "transfer = CountVectorizer(stop_words=[\"打算\",\"就是\"]) \n",
    "#实例化一个转换器类,\n",
    "# # stop_words=[\"打算\",\"就是\"],去除不想要的词\n",
    "data_new = transfer.fit_transform(cut_data)  #调用fit_transform()\n",
    "print(data_new)\n",
    "print(transfer.get_feature_names_out())\n",
    "print(data_new.toarray()) \n",
    "#构建成一个二维表：\n",
    "data=pd.DataFrame(data_new.toarray(),columns=transfer.get_feature_names_out())\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全部流程整合 输出结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用的包 导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 文本去噪\n",
    "去除一些无用的信息；特殊的符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_noise(text):\n",
    "    text = text.replace(u'\\xa0', u' ')      # 去除 \\xa0     不间断空白符 \n",
    "    text = text.replace(u'\\u3000', u' ')    # 去除 \\u3000   全角的空白符\n",
    "    \n",
    "    text = re.sub(r\"(回复)?(//)?\\s*@\\S*?\\s*(:| |$)\", \"\", text)  # 去除正文中的@和回复/转发中的用户名\n",
    "    text = re.sub(r\"\\[\\S+\\]\", \"\", text)     # 去除表情符号\n",
    "    # text = re.sub(r\"#\\S+#\", \"\", text)     # 去除话题内容\n",
    "    URL_REGEX = re.compile(\n",
    "        r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))',\n",
    "        re.IGNORECASE)\n",
    "    text = re.sub(URL_REGEX, \"\", text)      # 去除网址\n",
    "    \n",
    "    EMAIL_REGEX = re.compile(r\"[-a-z0-9_.]+@(?:[-a-z0-9]+\\.)+[a-z]{2,6}\", re.IGNORECASE)\n",
    "    text = re.sub(EMAIL_REGEX, \"\", text)    # 去除邮件 \n",
    "    \n",
    "    text = text.replace(\"转发微博\", \"\")      # 去除无意义的词语\n",
    "    text = text.replace(\"网页链接\", \"\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)        # 合并正文中过多的空格\n",
    "\n",
    "    text = re.sub(r\"\\d{2,4}年|\\d{1,2}月|\\d{1,2}日|\\d{1,2}时|\\d{1,2}分| \\d{1,2}点\", \"\", text) # 去除 日期 时间\n",
    "    text = re.sub(r\"\\d\", \"\", text)\n",
    "    text = re.sub('[a-zA-Z]',\"\", text)\n",
    "    \n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2/3 分词 过滤停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取停用词列表\n",
    "def get_stopword_list(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:    # \n",
    "        stopword_list = [word.strip('\\n') for word in f.readlines()]\n",
    "    return stopword_list\n",
    "\n",
    "# 分词 然后清除停用词语\n",
    "def clean_stopword(str, stopword_list):\n",
    "    result = []\n",
    "    word_list = jieba.cut(str)   # 分词后返回一个列表  jieba.cut(）   返回的是一个迭代器\n",
    "    for w in word_list:\n",
    "        if w not in stopword_list:\n",
    "            result.append(w)\n",
    "   \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.特征词选择\n",
    "\n",
    "统一处理和学习，这个就比较麻烦了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(lsls):\n",
    "    ls_str=[]\n",
    "    for s in lsls:\n",
    "        strs = ' '.join(s) \n",
    "        ls_str.append(strs)\n",
    "    \n",
    "    # TF-IDF\n",
    "    transfer = TfidfVectorizer() #实例化一个转换器类\n",
    "    data_new = transfer.fit_transform(ls_str) #调用fit_transform()\n",
    "    #构建成一个二维表：\n",
    "    df=pd.DataFrame(data_new.toarray(), columns=transfer.get_feature_names_out())\n",
    "    \n",
    "    withWeight = transfer.vocabulary_\n",
    "    return df, withWeight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原始数据及基本信息, 随机的500行，测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Data/完整数据_暴雨_交通.csv')\n",
    "df = data.sample(n=50, replace=False, random_state=1)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "tweets = df.loc[:, ['微博正文']]\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean_word'] = tweets['微博正文'].apply(clean_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = get_stopword_list('stopwords/hit_stopwords.txt')\n",
    "tweets['clean_stopwords'] = tweets['clean_word'].apply(clean_stopword, stopword_list=stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>一下</th>\n",
       "      <th>一分</th>\n",
       "      <th>一名</th>\n",
       "      <th>一场</th>\n",
       "      <th>一大</th>\n",
       "      <th>一定</th>\n",
       "      <th>一楼</th>\n",
       "      <th>一程</th>\n",
       "      <th>一线</th>\n",
       "      <th>一课</th>\n",
       "      <th>...</th>\n",
       "      <th>鱼塘</th>\n",
       "      <th>鹤壁</th>\n",
       "      <th>黄河路</th>\n",
       "      <th>黄色</th>\n",
       "      <th>黑车</th>\n",
       "      <th>黔西南州</th>\n",
       "      <th>默兹河</th>\n",
       "      <th>默默</th>\n",
       "      <th>鼓楼</th>\n",
       "      <th>鼓楼区</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.14623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1423 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        一下   一分       一名   一场   一大   一定   一楼   一程   一线   一课  ...   鱼塘   鹤壁  \\\n",
       "0  0.00000  0.0  0.00000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1  0.00000  0.0  0.00000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2  0.00000  0.0  0.00000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3  0.14623  0.0  0.14623  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4  0.00000  0.0  0.00000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "   黄河路   黄色   黑车  黔西南州  默兹河   默默   鼓楼  鼓楼区  \n",
       "0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 1423 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsls = tweets['clean_stopwords']\n",
    "df, withWeight = word2vec(lsls)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 949)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topK = 50\n",
    "downK = int(len(withWeight)/3)\n",
    "\n",
    "down_L = sorted(withWeight.items(), key=lambda item:item[1], reverse=False)\n",
    "top_L = sorted(withWeight.items(), key=lambda item:item[1], reverse=True)\n",
    "\n",
    "tags_del = list(map(lambda x: x[0], down_L[0:downK]))\n",
    "\n",
    "tags_sel = list(map(lambda x: x[0], top_L[0:topK]))\n",
    "\n",
    "df2 = df.drop(columns=tags_del, axis=1, inplace=False)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['鼓楼区', '鼓楼', '默默', '默兹河', '黔西南州', '黑车', '黄色', '黄河路', '鹤壁', '鱼塘', '魏城', '高铁', '高速公路', '高速', '高空槽', '高空作业', '高空', '高热量', '高温', '高栏', '高架', '高川', '高峰期', '高峰', '高处', '高位', '骤至', '骑着', '驾驶员', '驾驶', '驾车', '驻点', '驶离', '驱赶', '马路', '首报', '首义', '饱和', '食物', '飞仙', '风雨', '风险', '风景线', '风力', '频频', '频繁', '领导', '预防', '预计', '预警']\n"
     ]
    }
   ],
   "source": [
    "print(tags_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('Data/完整数据_暴雨_交通_pre_tweets_TFIDF.xlsx', index_col=0)\n",
    "df['Label'] = 1\n",
    "df.to_excel('Data/完整数据_暴雨_交通_pre_tweets_TFIDF_Label.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 报告数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line1 原始:\n",
      "#积水路段提醒##暴雨中的河南公安力量#1、金桥宾馆门前地下隧道积水严重，无法通行；2、金水路南阳路向北300米路东塌陷大坑；3、黄河路郑纺机地下道积水严重，无法通行。@平安郑州@河南交警@河南交通广播@郑州交通广播\n",
      "line2 去噪:\n",
      "#积水路段提醒##暴雨中的河南公安力量#、金桥宾馆门前地下隧道积水严重，无法通行；、金水路南阳路向北米路东塌陷大坑；、黄河路郑纺机地下道积水严重，无法通行。\n",
      "line3 分词:\n",
      "['#', '积水', '路段', '提醒', '##', '暴雨', '中', '的', '河南', '公安', '力量', '#', '、', '金桥', '宾馆', '门前', '地下隧道', '积水', '严重', '，', '无法', '通行', '；', '、', '金水路', '南阳', '路向', '北米', '路东', '塌陷', '大坑', '；', '、', '黄河路', '郑', '纺机', '地下道', '积水', '严重', '，', '无法', '通行', '。']\n",
      "line4 去停用词:\n",
      "['积水', '路段', '提醒', '##', '暴雨', '中', '河南', '公安', '力量', '金桥', '宾馆', '门前', '地下隧道', '积水', '严重', '无法', '通行', '金水路', '南阳', '路向', '北米', '路东', '塌陷', '大坑', '黄河路', '郑', '纺机', '地下道', '积水', '严重', '无法', '通行']\n"
     ]
    }
   ],
   "source": [
    "line1 = \"#积水路段提醒##暴雨中的河南公安力量#1、金桥宾馆门前地下隧道积水严重，无法通行；2、金水路南阳路向北300米路东塌陷大坑；3、黄河路郑纺机地下道积水严重，无法通行。@平安郑州@河南交警@河南交通广播@郑州交通广播\"\n",
    "\n",
    "print(f\"line1 原始:\\n{line1}\")\n",
    "\n",
    "line2 = clean_noise(line1)\n",
    "print(f\"line2 去噪:\\n{line2}\")\n",
    "\n",
    "line3 =jieba.lcut(line2)\n",
    "print(f\"line3 分词:\\n{line3}\")\n",
    "\n",
    "stopword = get_stopword_list('stopwords/hit_stopwords.txt')\n",
    "# 清除停用词语\n",
    "def clean_stopword(str, stopword_list):\n",
    "    result = []\n",
    "    word_list = jieba.cut(str)   # 分词后返回一个列表  jieba.cut(）   返回的是一个迭代器\n",
    "    for w in word_list:\n",
    "        if w not in stopword_list:\n",
    "            result.append(w)\n",
    "   \n",
    "    return result\n",
    "\n",
    "line4 =  clean_stopword(line2, stopword)\n",
    "print(f\"line4 去停用词:\\n{line4}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e481c8094450a362835654784466fc3605d9d17b45d80f6ff62525ad7933ac54"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
